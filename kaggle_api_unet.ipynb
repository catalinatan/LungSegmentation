{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # Uses ~/.kaggle/kaggle.json by default\n",
    "\n",
    "# List all files in the dataset\n",
    "files = api.dataset_list_files('nikhilpandey360/chest-xray-masks-and-labels').files\n",
    "\n",
    "# Specify the folders you want\n",
    "folders = ['CXR_png/', 'masks/', 'test/']\n",
    "\n",
    "# Download files from only those folders\n",
    "for file in files:\n",
    "    if any(file.name.startswith(folder) for folder in folders):\n",
    "        api.dataset_download_file(\n",
    "            'nikhilpandey360/chest-xray-masks-and-labels',\n",
    "            file_name=file.name,\n",
    "            path='desired_download_path',\n",
    "            force=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by:<br>\n",
    "https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inChannels, outChannels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inChannels, outChannels, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(outChannels, outChannels, kernel_size=3)\n",
    "    def forward(self, x):\n",
    "        return self.conv2(F.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Input: (1,1,256,256)\n",
    "    # Block1 → (1,64,256,256) → Pool → (1,64,128,128)\n",
    "    # Block2 → (1,128,128,128) → Pool → (1,128,64,64)\n",
    "    # Block3 → (1,256,64,64) → Pool → (1,256,32,32)\n",
    "    # Block4 → (1,512,32,32) → Pool → (1,512,16,16)\n",
    "    def __init__(self, channels=(1, 64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.encBlocks = nn.ModuleList(\n",
    "            [Block(channels[i], channels[i + 1])\n",
    "             for i in range(len(channels) - 1)])\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.encBlocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # Upconv1: (1,512,16,16) → (1,256,32,32)\n",
    "    # Concat → (1,512,32,32) → Block → (1,256,32,32)\n",
    "    # Upconv2: → (1,128,64,64)\n",
    "    # Concat → (1,256,64,64) → Block → (1,128,64,64)\n",
    "    # Upconv3: → (1,64,128,128)\n",
    "    # Concat → (1,128,128,128) → Block → (1,64,128,128)\n",
    "    # Upconv4: → (1,32,256,256)\n",
    "    # Final Conv: → (1,2,256,256)\n",
    "    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[i], channels[i+1], kernel_size=2, stride=2)\n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            Block(2 * channels[i+1], channels[i+1])  # Corrected line\n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "        self.channels = channels\n",
    "    def forward(self, x, encFeatures):\n",
    "        for i in range(len(self.channels) - 1):\n",
    "            # Upsampling\n",
    "            x = self.upconvs[i](x)\n",
    "            # Cropping\n",
    "            encFeat = self.crop(encFeatures[i], x)\n",
    "            # Concatenating encoder and decoder channels (along the\n",
    "            # 1st dimension)\n",
    "            x = torch.cat([x, encFeat], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    def crop(self, encFeatures, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "        return encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, inChannels=1, outChannels=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.finalConv = nn.Conv2d(64, outChannels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        encFeatures = self.encoder(x)\n",
    "        x = encFeatures[-1]\n",
    "        # Reverse the order of the encoder features to match the decoder\n",
    "        x = self.decoder(x, encFeatures[-2::-1])\n",
    "        return self.finalConv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by:<br>\n",
    "https://www.geeksforgeeks.org/converting-an-image-to-a-torch-tensor-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, size=(256, 256)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[:1, :, :] if x.shape[0] > 1 else x)  # Ensure 1 channel\n",
    "    ])\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_folder, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute valid pairs during initialization\n",
    "        self.valid_pairs = []\n",
    "        for image_path in self.image_files:\n",
    "            mask_name = image_path.name.replace(\".png\", \"_mask.png\")\n",
    "            mask_path = self.mask_folder / mask_name\n",
    "            if image_path.exists() and mask_path.exists():\n",
    "                self.valid_pairs.append((image_path, mask_path))\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.valid_pairs[idx]\n",
    "        image = Image.open(image_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, train_loader, criterion, optimizer):\n",
    "    for epoch in range(2):  # Use full 32 epochs\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.squeeze(1).long()  # Remove channel dimension\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Interpolate outputs to match labels size\n",
    "            if outputs.shape[-2:] != labels.shape[-2:]:\n",
    "                outputs = F.interpolate(outputs, labels.shape[-2:], mode='bilinear')\n",
    "\n",
    "            # Calculate loss (outputs: [N, 2, H, W], labels: [N, H, W])\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(net, test_loader):\n",
    "    net.eval()\n",
    "    true_positives, true_negatives, false_positives, false_negatives = 0, 0, 0, 0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Forward pass\n",
    "            print(f\"Inputs shape: {inputs.shape}\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            labels = labels.squeeze(1).long()  # Remove channel dimension\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Interpolate outputs to match labels size\n",
    "            if outputs.shape[-2:] != labels.shape[-2:]:\n",
    "                outputs = F.interpolate(outputs, labels.shape[-2:], mode='bilinear')\n",
    "\n",
    "            # __, predicted = torch.max(outputs.data, 1)\n",
    "            foreground_probs = torch.softmax(outputs, dim=1)[:, 1, :, :]  # shape [N, H, W]\n",
    "            threshold = 0.55\n",
    "            predicted = (foreground_probs > threshold).long()\n",
    "            matches = (predicted == labels)\n",
    "            correct += matches.sum().item()\n",
    "            total += labels.numel()\n",
    "            true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            true_negatives += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "            false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "        precision = (\n",
    "            true_positives / (true_positives + false_positives)\n",
    "            if true_positives + false_positives > 0\n",
    "            else 0\n",
    "        )\n",
    "        sensitivity = (\n",
    "            true_positives / (true_positives + false_negatives)\n",
    "            if true_positives + false_negatives > 0\n",
    "            else 0\n",
    "        )\n",
    "        specificity = (\n",
    "            true_negatives / (true_negatives + false_positives)\n",
    "            if true_negatives + false_positives > 0\n",
    "            else 0\n",
    "        )\n",
    "        print(f\"true_positives: {true_positives}\")\n",
    "        print(f\"true_negatives: {true_negatives}\")\n",
    "        print(f\"false_positives: {false_positives}\")\n",
    "        print(f\"false_negatives: {false_negatives}\")\n",
    "        print(f\"Accuracy: {correct / total:.2f}\")\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_folder = Path(\"chest_xray_dataset/Lung Segmentation/CXR_png\")\n",
    "    mask_folder = Path(\"chest_xray_dataset/Lung Segmentation/masks\")\n",
    "    image_files = list(image_folder.iterdir())\n",
    "    train_files, val_files = train_test_split(image_files, test_size=0.3, random_state=42)\n",
    "    print(f\"Train files: {len(train_files)}\")\n",
    "    print(f\"Val files: {len(val_files)}\")\n",
    "   \n",
    "    # Create dataset instance\n",
    "    train_dataset = ImageDataset(train_files, mask_folder, transform=resize_image)\n",
    "    val_dataset = ImageDataset(val_files, mask_folder, transform=resize_image)\n",
    "   \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Define the network\n",
    "    net = UNet()\n",
    "\n",
    "    # Class weights \n",
    "    class_weights = torch.tensor([0.2392, 0.7608])  # [Negative, Positive]\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use Adam optimizer\n",
    "\n",
    "    # #Train the network\n",
    "    train_network(net, train_loader, criterion, optimizer)\n",
    "    # Test the network\n",
    "    test_network(net, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
